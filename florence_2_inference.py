# -*- coding: utf-8 -*-
"""Florence-2 Inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ix3lPcauOgKtgHYebbyJZ3xG2xeQPwhz
"""

!pip install transformers flash_attn timm einops

# @title Imports

import io
import os
import re
import json
import torch
import html
import base64
import itertools

import numpy as np
import pandas as pd

from IPython.core.display import display, HTML
from torch.utils.data import Dataset, DataLoader

from transformers import (
    AutoModelForCausalLM,
    AutoProcessor,
    get_scheduler
)
from tqdm import tqdm
from typing import List, Dict, Any, Tuple, Generator

from PIL import Image

# Define paths
CROSSWALK2_DIR = "CROSSWALK2"
OUTPUT_CSV = "generated_captions.csv"

# Hugging Face token (ensure to keep it secure)
HF_TOKEN = "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"

# Load the fine-tuned model and processor from the Hugging Face repository
CHECKPOINT = "SatyaShodhaka/Florence-2-GDR"

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load model
model = AutoModelForCausalLM.from_pretrained(CHECKPOINT, use_auth_token=HF_TOKEN, trust_remote_code=True).to(DEVICE)
processor = AutoProcessor.from_pretrained(CHECKPOINT, use_auth_token=HF_TOKEN, trust_remote_code=True)

# Function to display image with captions
import matplotlib.pyplot as plt

def display_image_with_captions(image, generated_caption):
    plt.figure(figsize=(10, 5))
    plt.imshow(image)
    plt.axis('off')
    plt.title(f"Generated: {generated_caption}")
    plt.show()

import time

# Function to load image
def load_image(image_path):
    return Image.open(image_path).convert("RGB")

# Function to generate caption for an image
def generate_caption(image):
    # Perform inference using the pre-trained model
  inputs = processor(text="<DETAILED_CAPTION>", images=image, return_tensors="pt").to(DEVICE)
  generated_ids = model.generate(
      input_ids=inputs["input_ids"],
      pixel_values=inputs["pixel_values"],
      max_new_tokens=1024,
      num_beams=3
  )
  generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
  return generated_text

# Initialize list to store results
results = []
inference_times = []

# Iterate through images in the CROSSWALK2 directory
for image_file in os.listdir(CROSSWALK2_DIR):
    if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):
        image_path = os.path.join(CROSSWALK2_DIR, image_file)
        image = load_image(image_path)
        # Measure inference time
        start_time = time.time()
        caption = generate_caption(image)
        end_time = time.time()

        inference_time = end_time - start_time
        inference_times.append(inference_time)

        display_image_with_captions(image,caption)

        # Store result
        results.append({"image_id": image_file, "description": caption})

# Save results to a CSV file
df = pd.DataFrame(results)
df.to_csv(OUTPUT_CSV, index=False)

# Calculate and print average inference time
avg_inference_time = sum(inference_times) / len(inference_times)
print(f"Average Inference Time: {avg_inference_time:.3f} seconds")

print(f"Captions saved to {OUTPUT_CSV}")

# @title Loading Dataset

# Load the dataset
df = pd.read_csv('annotations2.csv')

# Function to load images
def load_image(image_id):

    return Image.open(f'/content/CROSSWALK2/{image_id}')

# Prepare datasets
test_images = [load_image(image_id) for image_id in df['image_id']]
test_captions = df['description'].tolist()

print(f'Test set size: {len(test_images)} images, {len(test_captions)} captions')

!pip install tqdm bert_score nltk

import matplotlib.pyplot as plt

# Evaluate the model
from bert_score import score
from nltk.translate.bleu_score import sentence_bleu

# Generate captions for test set
model.eval()
generated_captions = []
reference_captions = test_captions

with torch.no_grad():
    for idx, image in enumerate(tqdm(test_images, desc="Generating captions for test set")):
        inputs = processor(text="<DETAILED_CAPTION>", images=image, return_tensors="pt").to(DEVICE)
        generated_ids = model.generate(
            input_ids=inputs["input_ids"],
            pixel_values=inputs["pixel_values"],
            max_new_tokens=1024,
            num_beams=3
        )
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        generated_captions.append(generated_text)

# Compute BERTScore
P, R, F1 = score(generated_captions, reference_captions, lang="en", verbose=True)
print(f"BERTScore - Precision: {P.mean().item()}, Recall: {R.mean().item()}, F1: {F1.mean().item()}")

# Compute BLEU score
bleu_scores = []
for ref, gen in zip(reference_captions, generated_captions):
    ref_tokens = ref.split()
    gen_tokens = gen.split()
    score = sentence_bleu([ref_tokens], gen_tokens)
    bleu_scores.append(score)

avg_bleu_score = np.mean(bleu_scores)
print(f"BLEU Score: {avg_bleu_score}")

